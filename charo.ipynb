{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nezhalahnech/lfc-tweet-analysis/blob/master/charo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Trabajo final***\n",
        "# ***Nezha Lahnech***\n",
        "# ***datasets stroke***"
      ],
      "metadata": {
        "id": "nT6NZuH5Nory"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **importar librerias**"
      ],
      "metadata": {
        "id": "95toPkQUyXNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hCyeEWK8oe7U"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* *leer y descubrir el contenido del dataset storke*"
      ],
      "metadata": {
        "id": "9rOejq9bynHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/nezhalahnech/lfc-tweet-analysis/master/healthcare-dataset-stroke-data.csv', delimiter=',', na_values='?')\n",
        "# summarize the shape of the dataset\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpxD7jXdykqB",
        "outputId": "641877fd-63c4-438e-8a93-0dd0d436e0d8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5110, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize each variable\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gdDAx5rosOj",
        "outputId": "e3923471-61d8-4e32-e4c0-38cbebf01205"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5110 entries, 0 to 5109\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 5110 non-null   int64  \n",
            " 1   gender             5110 non-null   object \n",
            " 2   age                5110 non-null   float64\n",
            " 3   hypertension       5110 non-null   int64  \n",
            " 4   heart_disease      5110 non-null   int64  \n",
            " 5   ever_married       5110 non-null   object \n",
            " 6   work_type          5110 non-null   object \n",
            " 7   Residence_type     5110 non-null   object \n",
            " 8   avg_glucose_level  5110 non-null   float64\n",
            " 9   bmi                4909 non-null   float64\n",
            " 10  smoking_status     5110 non-null   object \n",
            " 11  stroke             5110 non-null   int64  \n",
            "dtypes: float64(3), int64(4), object(5)\n",
            "memory usage: 479.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "C5kBRRqLosSb",
        "outputId": "a5a7fa2a-a9bc-4ccb-b6bb-474ca40d9788"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-791cc4b6-ba8a-46d3-8c36-5abdf90b0586\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5110.000000</td>\n",
              "      <td>5110.000000</td>\n",
              "      <td>5110.000000</td>\n",
              "      <td>5110.000000</td>\n",
              "      <td>5110.000000</td>\n",
              "      <td>4909.000000</td>\n",
              "      <td>5110.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>36517.829354</td>\n",
              "      <td>43.226614</td>\n",
              "      <td>0.097456</td>\n",
              "      <td>0.054012</td>\n",
              "      <td>106.147677</td>\n",
              "      <td>28.893237</td>\n",
              "      <td>0.048728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>21161.721625</td>\n",
              "      <td>22.612647</td>\n",
              "      <td>0.296607</td>\n",
              "      <td>0.226063</td>\n",
              "      <td>45.283560</td>\n",
              "      <td>7.854067</td>\n",
              "      <td>0.215320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>67.000000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>55.120000</td>\n",
              "      <td>10.300000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>17741.250000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>77.245000</td>\n",
              "      <td>23.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>36932.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91.885000</td>\n",
              "      <td>28.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>54682.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>114.090000</td>\n",
              "      <td>33.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>72940.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>271.740000</td>\n",
              "      <td>97.600000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-791cc4b6-ba8a-46d3-8c36-5abdf90b0586')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-791cc4b6-ba8a-46d3-8c36-5abdf90b0586 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-791cc4b6-ba8a-46d3-8c36-5abdf90b0586');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 id          age  ...          bmi       stroke\n",
              "count   5110.000000  5110.000000  ...  4909.000000  5110.000000\n",
              "mean   36517.829354    43.226614  ...    28.893237     0.048728\n",
              "std    21161.721625    22.612647  ...     7.854067     0.215320\n",
              "min       67.000000     0.080000  ...    10.300000     0.000000\n",
              "25%    17741.250000    25.000000  ...    23.500000     0.000000\n",
              "50%    36932.000000    45.000000  ...    28.100000     0.000000\n",
              "75%    54682.000000    61.000000  ...    33.100000     0.000000\n",
              "max    72940.000000    82.000000  ...    97.600000     1.000000\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "29t2qm82osWQ",
        "outputId": "d940e288-cad0-40e5-8ee0-9364cc8c7266"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4784adeb-4e7b-4bcc-88af-2e9287804d98\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>ever_married</th>\n",
              "      <th>work_type</th>\n",
              "      <th>Residence_type</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>smoking_status</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9046</td>\n",
              "      <td>Male</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>228.69</td>\n",
              "      <td>36.6</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51676</td>\n",
              "      <td>Female</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>202.21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31112</td>\n",
              "      <td>Male</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Rural</td>\n",
              "      <td>105.92</td>\n",
              "      <td>32.5</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60182</td>\n",
              "      <td>Female</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>171.23</td>\n",
              "      <td>34.4</td>\n",
              "      <td>smokes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1665</td>\n",
              "      <td>Female</td>\n",
              "      <td>79.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>174.12</td>\n",
              "      <td>24.0</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4784adeb-4e7b-4bcc-88af-2e9287804d98')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4784adeb-4e7b-4bcc-88af-2e9287804d98 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4784adeb-4e7b-4bcc-88af-2e9287804d98');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      id  gender   age  ...   bmi   smoking_status stroke\n",
              "0   9046    Male  67.0  ...  36.6  formerly smoked      1\n",
              "1  51676  Female  61.0  ...   NaN     never smoked      1\n",
              "2  31112    Male  80.0  ...  32.5     never smoked      1\n",
              "3  60182  Female  49.0  ...  34.4           smokes      1\n",
              "4   1665  Female  79.0  ...  24.0     never smoked      1\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Borrado de duplicados."
      ],
      "metadata": {
        "id": "5BsrLAsNy37O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* *Lo primero que debemos hacer es comprobar si tenemos instancias duplicadas y si las hubiera eliminarlas.*"
      ],
      "metadata": {
        "id": "rP6pKEDXzKzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVwhjXbJpQKd",
        "outputId": "44d653d0-77b3-4d87-ff14-474e058746f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*en este No tenemos instancias repetidas*"
      ],
      "metadata": {
        "id": "keDFpjtHzYOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tratamiento de valores nulos"
      ],
      "metadata": {
        "id": "1ty5FEa9zkqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos cuántos valores nulos hay. Si contienen muchos valores nulos será preferible usar alguna técnica de imputación y asignarles un valor."
      ],
      "metadata": {
        "id": "KdsU9e6qz4un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().any(axis=1).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfFX4yN5pQOY",
        "outputId": "45637de4-ebb9-4b02-f074-5bec1407441d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQFJkTiapQSR",
        "outputId": "51e92f31-4ad8-4003-a206-f3d257617d65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                     0\n",
              "gender                 0\n",
              "age                    0\n",
              "hypertension           0\n",
              "heart_disease          0\n",
              "ever_married           0\n",
              "work_type              0\n",
              "Residence_type         0\n",
              "avg_glucose_level      0\n",
              "bmi                  201\n",
              "smoking_status         0\n",
              "stroke                 0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Borrado de instancias con valores nulos."
      ],
      "metadata": {
        "id": "zcXfFTbx0hjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuamos borrando las instancias con valores nulos."
      ],
      "metadata": {
        "id": "prwTFM4f0qiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.dropna(inplace=False)"
      ],
      "metadata": {
        "id": "eWJ0bjx-osaN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean.shape)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPMqVXBSoshE",
        "outputId": "ac6972be-e97f-43e9-bb84-edbe069b308c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4909, 12)\n",
            "(5110, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vemos como hemos eliminado las instancias que contenían alguna columna con valores null.*"
      ],
      "metadata": {
        "id": "It3dAevs04Eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Imputación de valores nulos."
      ],
      "metadata": {
        "id": "jBX2t_-Z1KHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creamos una variable X que tendrá las características de nuestros datos menos la variable objetivo\n",
        "X = df.drop('stroke', axis=1)\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']"
      ],
      "metadata": {
        "id": "Guufr-h3qT9Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from numpy import isnan\n",
        "\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "Xtrans = imputer.fit_transform(X)"
      ],
      "metadata": {
        "id": "GmwbKTS0p2zF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Con el dataset df_clean (instancias con nulos elimiminadas) obtenemos X e y\n",
        "X = df_clean.drop('stroke', axis=1).astype('str')\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df_clean['stroke']\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# one-hot encode input variables\n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "pipeline = Pipeline(steps=[('oh', onehot_encoder ), ('m', model)])\n",
        "# define model evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRuHe66YAUk9",
        "outputId": "e6dcb7dc-ee82-4ca9-d5ad-a33dc6a5e40e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.956 (0.001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Con el dataset df (con nulos): imputación del más frecuente y onehot encoder (instancias con nulos elimiminadas) obtenemos X e y\n",
        "X = df.drop('stroke', axis=1).astype('str')\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# one-hot encode input variables\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore')   #.astype('str')\n",
        "\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "pipeline = Pipeline(steps=[('i', imputer), ('oh', onehot_encoder ), ('m', model)])\n",
        "# define model evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2aSiSLzAUhV",
        "outputId": "30f657c6-cc22-4e41-8355-0b2e7401b4d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.950 (0.002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Con el dataset df (con nulos), onehot encoder y después imputación (instancias con nulos elimiminadas) obtenemos X e y\n",
        "X = df.drop('stroke', axis=1).astype('str')  #ojo, mostrar que si no es 'str' o 'int' o 'category' da nan.\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# one-hot encode input variables\n",
        "imputer = SimpleImputer(strategy='median') \n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "pipeline = Pipeline(steps=[('oh', onehot_encoder ), ('i', imputer), ('m', model)])\n",
        "# define model evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0mNvmdzAUb7",
        "outputId": "89a13188-5d02-46f7-98e6-d5cc4170b66d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.950 (0.002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n",
        "print(df.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46XLSuoFp3Eb",
        "outputId": "56e8cca8-9dae-475a-d2a3-740cdec2c929"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5110 entries, 0 to 5109\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 5110 non-null   int64  \n",
            " 1   gender             5110 non-null   object \n",
            " 2   age                5110 non-null   float64\n",
            " 3   hypertension       5110 non-null   int64  \n",
            " 4   heart_disease      5110 non-null   int64  \n",
            " 5   ever_married       5110 non-null   object \n",
            " 6   work_type          5110 non-null   object \n",
            " 7   Residence_type     5110 non-null   object \n",
            " 8   avg_glucose_level  5110 non-null   float64\n",
            " 9   bmi                4909 non-null   float64\n",
            " 10  smoking_status     5110 non-null   object \n",
            " 11  stroke             5110 non-null   int64  \n",
            "dtypes: float64(3), int64(4), object(5)\n",
            "memory usage: 479.2+ KB\n",
            "None\n",
            "      id  gender   age  ...   bmi   smoking_status stroke\n",
            "0   9046    Male  67.0  ...  36.6  formerly smoked      1\n",
            "1  51676  Female  61.0  ...   NaN     never smoked      1\n",
            "\n",
            "[2 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['id']=df['id'].astype(dtype='category')   #Uso esta vez el tipo category\n",
        "df['gender']=df['gender'].astype(dtype='category')\n",
        "df['age']=df['age'].astype(dtype='category')\n",
        "df['hypertension']=df['hypertension'].astype(dtype='category')\n",
        "df['heart_disease']=df['heart_disease'].astype(dtype='category')\n",
        "df['ever_married']=df['ever_married'].astype(dtype='category')\n",
        "df['work_type']=df['work_type'].astype(dtype='category')\n",
        "df['Residence_type']=df['Residence_type'].astype(dtype='category')\n",
        "df['avg_glucose_level']=df['avg_glucose_level'].astype(dtype='category')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbKlrST_AUOj",
        "outputId": "23b6eeb1-cd50-4173-f126-f9475fde96b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5110 entries, 0 to 5109\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype   \n",
            "---  ------             --------------  -----   \n",
            " 0   id                 5110 non-null   category\n",
            " 1   gender             5110 non-null   category\n",
            " 2   age                5110 non-null   category\n",
            " 3   hypertension       5110 non-null   category\n",
            " 4   heart_disease      5110 non-null   category\n",
            " 5   ever_married       5110 non-null   category\n",
            " 6   work_type          5110 non-null   category\n",
            " 7   Residence_type     5110 non-null   category\n",
            " 8   avg_glucose_level  5110 non-null   category\n",
            " 9   bmi                4909 non-null   float64 \n",
            " 10  smoking_status     5110 non-null   object  \n",
            " 11  stroke             5110 non-null   int64   \n",
            "dtypes: category(9), float64(1), int64(1), object(1)\n",
            "memory usage: 509.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('stroke', axis=1)\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']\n",
        "\n",
        "import numpy as np \n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "numeric_features = [\"bmi\", \"smoking_status\", \"stroke\" ]\n",
        "numeric_transformer = Pipeline(\n",
        "    #steps=[(\"imputer\", KNNImputer()), (\"scaler\", MinMaxScaler())]  #Le añado un MinMaxScaler --> 86.2%\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", MinMaxScaler())] #--> 86,4%\n",
        ")\n",
        "\n",
        "categorical_features = [\"gender\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"avg_glucose_level\"]  \n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  (\"oneHotE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), ('m', model)])\n",
        "\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xDW1dQ5AUJ8",
        "outputId": "07e88adc-b9bd-410e-c0bb-6d278c02a024"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Con el dataset df_clean (instancias con nulos elimiminadas) obtenemos X e y\n",
        "X = df_clean.drop('stroke', axis=1).astype('str')\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df_clean['stroke']\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# one-hot encode input variables\n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "pipeline = Pipeline(steps=[('oh', onehot_encoder ), ('m', model)])\n",
        "# define model evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "id": "AW3BqFnDp3Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04656244-1234-4df0-9337-a4f6ebec0bc7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.956 (0.001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solo un train_tes_split para ver qué ocurre\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "X = df_clean.drop('stroke', axis=1)\n",
        "y = df_clean['stroke']\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "oneHot=OneHotEncoder(handle_unknown='ignore')\n",
        "X=oneHot.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                        train_size   = 0.8,\n",
        "                                        random_state = 50,\n",
        "                                        shuffle      = True\n",
        "                                    )\n",
        "\n",
        "logistic_regression = LogisticRegression(max_iter = 1000000000 )\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred_logistic_regression = logistic_regression.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred_logistic_regression))"
      ],
      "metadata": {
        "id": "kZRK1ghep3O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee24e5f-23ba-4d94-cf91-3a9325e808b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9582484725050916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "X = df_clean.drop('stroke', axis=1)\n",
        "y = df_clean['stroke']\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "oneHot=OneHotEncoder(handle_unknown='ignore')\n",
        "X=oneHot.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                        train_size   = 0.8,\n",
        "                                        random_state = 50,\n",
        "                                        shuffle      = True\n",
        "                                    )\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=50)\n",
        "decision_tree.fit(X_train, y_train)\n",
        "y_pred_decision_tree = decision_tree.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred_decision_tree))"
      ],
      "metadata": {
        "id": "AcIdLVRXp3SG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56aecfcc-a067-4d2e-ce49-5ac90878bc00"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9501018329938901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Con el dataset df_clean (instancias con nulos elimiminadas) obtenemos X e y\n",
        "X = df_clean.drop('stroke', axis=1).astype('str')\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df_clean['stroke']\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# one-hot encode input variables\n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "# define the model\n",
        "model = RandomForestClassifier(random_state=50)\n",
        "\n",
        "pipeline = Pipeline(steps=[('oh', onehot_encoder ), ('m', model)])\n",
        "# define model evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS_g1vpgBMaT",
        "outputId": "0814eb41-0c03-4189-f498-a3dc031957dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.957 (0.001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_clean.drop('stroke', axis=1)\n",
        "y = df_clean['stroke']\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "oneHot=OneHotEncoder(handle_unknown='ignore')\n",
        "X=oneHot.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                        train_size   = 0.8,\n",
        "                                        random_state = 50,\n",
        "                                        shuffle      = True\n",
        "                                    )\n",
        "\n",
        "\n",
        "random_forest = RandomForestClassifier(random_state=50)\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred_random_forest = random_forest.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred_random_forest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQFpIwJ2BMdw",
        "outputId": "752baa88-f043-4fb0-eeb5-05e446e35698"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9582484725050916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('stroke', axis=1)\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']\n",
        "\n",
        "import numpy as np \n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "numeric_features = [\"bmi\", \"smoking_status\", \"stroke\"]\n",
        "numeric_transformer = Pipeline(\n",
        "    #steps=[(\"imputer\", KNNImputer())]  #0.874\n",
        "    #steps=[(\"imputer\", KNNImputer()), (\"scaler\", MinMaxScaler())]  #0.874\n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))] #0.879\n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())] #0.879\n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", MinMaxScaler())] #0.876\n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())] #0.876\n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", RobustScaler(quantile_range=(15.0, 100-15)))] #0.877\n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", RobustScaler(quantile_range=(25.0, 75)))] #0.877\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))] #0.879\n",
        "    #steps=[(\"imputer\", IterativeImputer(imputation_order='random', initial_strategy='median')), (\"scaler\", RobustScaler())] #0.876\n",
        ")\n",
        "\n",
        "categorical_features = [\"gender\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"avg_glucose_level\"]  \n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  (\"oneHotE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = RandomForestClassifier(random_state=50)\n",
        "\n",
        "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), ('m', model)])\n",
        "\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-CWePHxBMhS",
        "outputId": "eadab80f-7488-4e8c-e16f-a92de4f3f8b8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('stroke', axis=1)\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']"
      ],
      "metadata": {
        "id": "rGMBkyhKBMk2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.isna().sum() #Comprobamos que están los nulos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mUE9Lr_BMn5",
        "outputId": "9d8f2b70-9176-44f0-f3de-93899cbe84e9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                     0\n",
              "gender                 0\n",
              "age                    0\n",
              "hypertension           0\n",
              "heart_disease          0\n",
              "ever_married           0\n",
              "work_type              0\n",
              "Residence_type         0\n",
              "avg_glucose_level      0\n",
              "bmi                  201\n",
              "smoking_status         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "fig = df.hist(xlabelsize=4, ylabelsize=4)\n",
        "#[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "O0sDabbWBMrV",
        "outputId": "f6d52fb6-9523-48d5-e529-4da18668419e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('stroke', axis=1)\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']\n",
        "\n",
        "import numpy as np \n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
        "\n",
        "numeric_features = [\"bmi\", \"smoking_status\", \"stroke\"]\n",
        "numeric_transformer = Pipeline(\n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), ('scaler', MinMaxScaler(feature_range=(1, 2))),('power',PowerTransformer(method='box-cox'))] \n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), ('scaler', MinMaxScaler(feature_range=(0, 1))),('power',PowerTransformer(method='box-cox'))] #nan \n",
        "    #steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), ('scaler', StandardScaler()),('power',PowerTransformer(method='yeo-johnson'))] #powerTransforer no aporta nada al usar Random Forest\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))] \n",
        ")\n",
        "\n",
        "categorical_features = [\"gender\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"avg_glucose_level\"]  \n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  (\"oneHotE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "        \n",
        "    ]\n",
        ")\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = RandomForestClassifier(random_state=50)\n",
        "\n",
        "# evaluate model\n",
        "for k_features in [2,5,7,10, 'all']:\n",
        "    sel = SelectKBest(score_func=mutual_info_classif, k=k_features)  #Mutual information\n",
        "    #gauss = PowerTransformer(method='yeo-johnson')\n",
        "    \n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               #('gauss', gauss),  #No aportará nada a este estimador.\n",
        "                               ('sel', sel),\n",
        "                               ('m', model)])\n",
        "    # evaluate the pipeline\n",
        "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "    n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "    print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpZg6bWzBM2r",
        "outputId": "7c968a19-d205-44a7-d156-42042d4ae870"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n",
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('stroke', axis=1)\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']\n",
        "\n",
        "import numpy as np \n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "numeric_features = [\"bmi\", \"smoking_status\", \"stroke\"]\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))] #0.879\n",
        ")\n",
        "\n",
        "categorical_features = [\"gender\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"avg_glucose_level\"]  \n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  (\"oneHotE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = RandomForestClassifier(random_state=50)\n",
        "\n",
        "# evaluate model\n",
        "for k_features in [2,5,7,10,15]:\n",
        "    sel = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=k_features)\n",
        "\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('sel', sel),\n",
        "                               ('m', model)])\n",
        "    # evaluate the pipeline\n",
        "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "    n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "    print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQqZgpMkBxKN",
        "outputId": "a3414591-8c0a-4b88-9748-9c49a232e133"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n",
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('stroke', axis=1)\n",
        "#creamos una variable y que tendrá sólo la variable objetivo\n",
        "y = df['stroke']\n",
        "\n",
        "import numpy as np \n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
        "\n",
        "numeric_features = [\"bmi\", \"smoking_status\", \"stroke\"]\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))] \n",
        ")\n",
        "\n",
        "categorical_features = [\"gender\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"avg_glucose_level\"]  \n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  (\"oneHotE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "        \n",
        "    ]\n",
        ")\n",
        "# ordinal encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = RandomForestClassifier(random_state=50)\n",
        "\n",
        "# evaluate model\n",
        "sel = SelectKBest(score_func=mutual_info_classif, k=k_features)  #Mutual information\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('sel', sel),\n",
        "                           ('m', model)])\n",
        "# evaluate the pipeline\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl1LWwQAB4NF",
        "outputId": "5a03b107-f0b3-4a52-9efc-f0cbc9adc0cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 433, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'stroke'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 672, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 352, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 441, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    }
  ]
}